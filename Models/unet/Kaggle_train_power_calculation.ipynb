{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swlvwrbUNYYB",
        "outputId": "ee12bfe2-11ad-46ae-81d7-035015b0c4dc"
      },
      "outputs": [],
      "source": [
        "!pip install eco2ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p951OBxknZE8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E:\\Code\\Spiking-Visual-attention-for-Medical-image-segmentation\\Models\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/kaggle/input/unet/pytorch/unet/1')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils import (\n",
        "    save_checkpoint,\n",
        "    load_checkpoint,\n",
        "    get_loaders,\n",
        "    check_accuracy,\n",
        "    save_predictions_as_imgs,\n",
        "    summarize_eco2ai_log,\n",
        "    )\n",
        "\n",
        "from model import UNET\n",
        "\n",
        "from eco2ai import Tracker\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JEuqu_vatU5",
        "outputId": "8af92515-abd9-4848-ab82-a23e7ff4bdf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [32, 64, 256, 256]             576\n",
            "       BatchNorm2d-2         [32, 64, 256, 256]             128\n",
            "              ReLU-3         [32, 64, 256, 256]               0\n",
            "            Conv2d-4         [32, 64, 256, 256]          36,864\n",
            "       BatchNorm2d-5         [32, 64, 256, 256]             128\n",
            "              ReLU-6         [32, 64, 256, 256]               0\n",
            "        DoubleConv-7         [32, 64, 256, 256]               0\n",
            "         MaxPool2d-8         [32, 64, 128, 128]               0\n",
            "            Conv2d-9        [32, 128, 128, 128]          73,728\n",
            "      BatchNorm2d-10        [32, 128, 128, 128]             256\n",
            "             ReLU-11        [32, 128, 128, 128]               0\n",
            "           Conv2d-12        [32, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-13        [32, 128, 128, 128]             256\n",
            "             ReLU-14        [32, 128, 128, 128]               0\n",
            "       DoubleConv-15        [32, 128, 128, 128]               0\n",
            "        MaxPool2d-16          [32, 128, 64, 64]               0\n",
            "           Conv2d-17          [32, 256, 64, 64]         294,912\n",
            "      BatchNorm2d-18          [32, 256, 64, 64]             512\n",
            "             ReLU-19          [32, 256, 64, 64]               0\n",
            "           Conv2d-20          [32, 256, 64, 64]         589,824\n",
            "      BatchNorm2d-21          [32, 256, 64, 64]             512\n",
            "             ReLU-22          [32, 256, 64, 64]               0\n",
            "       DoubleConv-23          [32, 256, 64, 64]               0\n",
            "        MaxPool2d-24          [32, 256, 32, 32]               0\n",
            "           Conv2d-25          [32, 512, 32, 32]       1,179,648\n",
            "      BatchNorm2d-26          [32, 512, 32, 32]           1,024\n",
            "             ReLU-27          [32, 512, 32, 32]               0\n",
            "           Conv2d-28          [32, 512, 32, 32]       2,359,296\n",
            "      BatchNorm2d-29          [32, 512, 32, 32]           1,024\n",
            "             ReLU-30          [32, 512, 32, 32]               0\n",
            "       DoubleConv-31          [32, 512, 32, 32]               0\n",
            "        MaxPool2d-32          [32, 512, 16, 16]               0\n",
            "           Conv2d-33         [32, 1024, 16, 16]       4,718,592\n",
            "      BatchNorm2d-34         [32, 1024, 16, 16]           2,048\n",
            "             ReLU-35         [32, 1024, 16, 16]               0\n",
            "           Conv2d-36         [32, 1024, 16, 16]       9,437,184\n",
            "      BatchNorm2d-37         [32, 1024, 16, 16]           2,048\n",
            "             ReLU-38         [32, 1024, 16, 16]               0\n",
            "       DoubleConv-39         [32, 1024, 16, 16]               0\n",
            "  ConvTranspose2d-40          [32, 512, 32, 32]       2,097,664\n",
            "           Conv2d-41          [32, 512, 32, 32]       4,718,592\n",
            "      BatchNorm2d-42          [32, 512, 32, 32]           1,024\n",
            "             ReLU-43          [32, 512, 32, 32]               0\n",
            "           Conv2d-44          [32, 512, 32, 32]       2,359,296\n",
            "      BatchNorm2d-45          [32, 512, 32, 32]           1,024\n",
            "             ReLU-46          [32, 512, 32, 32]               0\n",
            "       DoubleConv-47          [32, 512, 32, 32]               0\n",
            "  ConvTranspose2d-48          [32, 256, 64, 64]         524,544\n",
            "           Conv2d-49          [32, 256, 64, 64]       1,179,648\n",
            "      BatchNorm2d-50          [32, 256, 64, 64]             512\n",
            "             ReLU-51          [32, 256, 64, 64]               0\n",
            "           Conv2d-52          [32, 256, 64, 64]         589,824\n",
            "      BatchNorm2d-53          [32, 256, 64, 64]             512\n",
            "             ReLU-54          [32, 256, 64, 64]               0\n",
            "       DoubleConv-55          [32, 256, 64, 64]               0\n",
            "  ConvTranspose2d-56        [32, 128, 128, 128]         131,200\n",
            "           Conv2d-57        [32, 128, 128, 128]         294,912\n",
            "      BatchNorm2d-58        [32, 128, 128, 128]             256\n",
            "             ReLU-59        [32, 128, 128, 128]               0\n",
            "           Conv2d-60        [32, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-61        [32, 128, 128, 128]             256\n",
            "             ReLU-62        [32, 128, 128, 128]               0\n",
            "       DoubleConv-63        [32, 128, 128, 128]               0\n",
            "  ConvTranspose2d-64         [32, 64, 256, 256]          32,832\n",
            "           Conv2d-65         [32, 64, 256, 256]          73,728\n",
            "      BatchNorm2d-66         [32, 64, 256, 256]             128\n",
            "             ReLU-67         [32, 64, 256, 256]               0\n",
            "           Conv2d-68         [32, 64, 256, 256]          36,864\n",
            "      BatchNorm2d-69         [32, 64, 256, 256]             128\n",
            "             ReLU-70         [32, 64, 256, 256]               0\n",
            "       DoubleConv-71         [32, 64, 256, 256]               0\n",
            "           Conv2d-72          [32, 1, 256, 256]              65\n",
            "================================================================\n",
            "Total params: 31,036,481\n",
            "Trainable params: 31,036,481\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 8.00\n",
            "Forward/backward pass size (MB): 29744.00\n",
            "Params size (MB): 118.39\n",
            "Estimated Total Size (MB): 29870.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNET(in_channel=1, out_channel=1).to(device)\n",
        "summary(model, input_size=(1, 256, 256), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "haTEVFwi41yC"
      },
      "outputs": [],
      "source": [
        "IMG_DIR      =  \"/kaggle/input/braindataset/Modified_3_Brain_Tumor_Segmentation/images\"\n",
        "MASK_DIR     =  \"/kaggle/input/braindataset/Modified_3_Brain_Tumor_Segmentation/masks\"\n",
        "VAL_IMG_DIR  =  \"/kaggle/input/braindataset/Modified_3_Brain_Tumor_Segmentation/val_images\"\n",
        "VAL_MASK_DIR =  \"/kaggle/input/braindataset/Modified_3_Brain_Tumor_Segmentation/val_masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LYRuU4iGnZE-"
      },
      "outputs": [],
      "source": [
        "Device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "Learning_rate = 1e-3 # 1e-4 originally\n",
        "Batch_size  = 32\n",
        "num_epochs  = 60\n",
        "num_workers = 4\n",
        "IMAGE_HEIGHT = 256 # 512 originally\n",
        "IMAGE_WIDTH  = 256 # 512 originally\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "CHECKPOINT_NAME = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wtgaXcl0nZE_"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    tracker = Tracker(\n",
        "        project_name=\"U_Net_Training_Project\",\n",
        "        experiment_description=\"Training U_Net Model\",\n",
        "        file_name=\"U_Net_eco2ai_logs.csv\",\n",
        "        alpha_2_code=\"EG\",\n",
        "    )\n",
        "    tracker.start()\n",
        "\n",
        "    loop = tqdm(loader)\n",
        "    running_loss=0\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=Device)\n",
        "        targets = targets.float().unsqueeze(1).to(device=Device)\n",
        "\n",
        "        with torch.amp.autocast(device_type=Device):\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    tracker.stop()\n",
        "\n",
        "    return running_loss/len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1XBWepI1nZFA"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.1),\n",
        "            A.Normalize(\n",
        "                mean=[0.0],\n",
        "                std=[1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Normalize(\n",
        "                mean=[0.0],\n",
        "                std=[1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "    model = UNET(in_channel=1, out_channel=1).to(Device)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Learning_rate)\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "    # scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        IMG_DIR,\n",
        "        MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        Batch_size,\n",
        "        train_transform,\n",
        "        val_transform,\n",
        "        num_workers,\n",
        "        PIN_MEMORY,\n",
        "    )\n",
        "\n",
        "    train_losses=[]\n",
        "    val_dice_scores=[]\n",
        "    val_accs=[]\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(model=model, optimizer=optimizer, checkpoint_name=CHECKPOINT_NAME)\n",
        "        check_accuracy(val_loader, model, device=Device)\n",
        "\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "BGMBxUSCnZFB",
        "outputId": "e74904a9-9a1e-457e-b363-e23476d7fc54"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "YFiPmYWANYYF",
        "outputId": "5c7df2c0-9b56-4b0c-e73d-8c5abf7529c0"
      },
      "outputs": [],
      "source": [
        "summarize_eco2ai_log(\"U_Net_eco2ai_logs.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
